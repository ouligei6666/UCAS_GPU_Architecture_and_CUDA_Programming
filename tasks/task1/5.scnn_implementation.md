# scnn实现
## 了解scnn的架构
作业一发布页已经强调了结构Conv -> IF -> Pool -> Conv -> IF -> Pool -> Flatten -> FC -> IF -> FC -> IF -> FC。要掌握结构可以仔细阅读作业提供的训练程序[scnn_training.py](./needed_files/scnn_training.py);可以阅读scnn官方文档 https://spikingjelly.readthedocs.io/zh-cn/latest/activation_based/conv_fashion_mnist.html

目标就是用cuda程序实现卷积、激活、池化 等结构中出现的组件，把他们组转起来。

## 编辑cuda

在模板程序[scnn_predicting.cu](./needed_files/scnn_predicting.cu)中可以进行编辑的位置在71行到99行和162行到170行，也就是scnn的实现和scnn的调用部分。

## 这里给出一种实现
```C++
// ===================================================================================
// CUDA Kernel Functions for SCNN Inference
// ===================================================================================

// IF神经元kernel - 积分-发放神经元
__global__ void if_neuron_kernel(float* input, float* output, float* membrane_potential, 
                                int size, float threshold, float reset_potential) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float potential = membrane_potential[idx] + input[idx];
        
        if (potential >= threshold) {
            output[idx] = 1.0f;
            membrane_potential[idx] = reset_potential;
        } else {
            output[idx] = 0.0f;
            membrane_potential[idx] = potential;
        }
    }
}

// 2D卷积kernel
__global__ void conv2d_kernel(float* input, float* weight, float* bias, float* output,
                             int input_h, int input_w, int input_c, int output_c,
                             int kernel_size, int stride, int padding) {
    int out_h = (input_h + 2 * padding - kernel_size) / stride + 1;
    int out_w = (input_w + 2 * padding - kernel_size) / stride + 1;
    
    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_output = out_h * out_w * output_c;
    
    if (out_idx < total_output) {
        int out_c = out_idx / (out_h * out_w);
        int out_y = (out_idx % (out_h * out_w)) / out_w;
        int out_x = out_idx % out_w;
        
        float sum = bias[out_c];
        
        for (int in_c = 0; in_c < input_c; in_c++) {
            for (int ky = 0; ky < kernel_size; ky++) {
                for (int kx = 0; kx < kernel_size; kx++) {
                    int in_y = out_y * stride + ky - padding;
                    int in_x = out_x * stride + kx - padding;
                    
                    if (in_y >= 0 && in_y < input_h && in_x >= 0 && in_x < input_w) {
                        int input_idx = in_c * input_h * input_w + in_y * input_w + in_x;
                        int weight_idx = out_c * input_c * kernel_size * kernel_size + 
                                       in_c * kernel_size * kernel_size + 
                                       ky * kernel_size + kx;
                        sum += input[input_idx] * weight[weight_idx];
                    }
                }
            }
        }
        
        output[out_idx] = sum;
    }
}

// MaxPool2d kernel
__global__ void maxpool2d_kernel(float* input, float* output,
                                 int input_h, int input_w, int input_c,
                                 int kernel_size, int stride) {
    int out_h = input_h / stride;
    int out_w = input_w / stride;
    
    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_output = out_h * out_w * input_c;
    
    if (out_idx < total_output) {
        int c = out_idx / (out_h * out_w);
        int out_y = (out_idx % (out_h * out_w)) / out_w;
        int out_x = out_idx % out_w;
        
        float max_val = -INFINITY;
        
        for (int ky = 0; ky < kernel_size; ky++) {
            for (int kx = 0; kx < kernel_size; kx++) {
                int in_y = out_y * stride + ky;
                int in_x = out_x * stride + kx;
                int input_idx = c * input_h * input_w + in_y * input_w + in_x;
                max_val = fmaxf(max_val, input[input_idx]);
            }
        }
        
        output[out_idx] = max_val;
    }
}

// 全连接层kernel
__global__ void linear_kernel(float* input, float* weight, float* bias, float* output,
                            int input_size, int output_size) {
    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (out_idx < output_size) {
        float sum = bias[out_idx];
        
        for (int in_idx = 0; in_idx < input_size; in_idx++) {
            int weight_idx = out_idx * input_size + in_idx;
            sum += input[in_idx] * weight[weight_idx];
        }
        
        output[out_idx] = sum;
    }
}

// 脉冲编码kernel - 将输入图像转换为脉冲
__global__ void spike_encoding_kernel(float* input, float* output, int size, float threshold) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] = (input[idx] > threshold) ? 1.0f : 0.0f;
    }
}

// 时间步平均kernel
__global__ void time_average_kernel(float* accumulated, float* output, int size, int time_steps) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] = accumulated[idx] / time_steps;
    }
}

// 累加kernel
__global__ void accumulate_kernel(float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] += input[idx];
    }
}

// 预测kernel - 找到最大值的索引
__global__ void prediction_kernel(float* input, int* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float max_val = -INFINITY;
        int max_idx = 0;
        
        for (int i = 0; i < 10; i++) {  // 10个类别
            if (input[idx * 10 + i] > max_val) {
                max_val = input[idx * 10 + i];
                max_idx = i;
            }
        }
        
        output[idx] = max_idx;
    }
}

// ===================================================================================
// CUDA Kernel Functions End
// ===================================================================================

std::vector<int> scnn_inference(
    const std::vector<std::vector<float>>& images,
    // Device pointers for parameters
    float* d_conv1_w, float* d_conv1_b, float* d_conv2_w, float* d_conv2_b,
    float* d_fc1_w,   float* d_fc1_b,   float* d_fc2_w,   float* d_fc2_b,
    float* d_fc3_w,   float* d_fc3_b
    // YOU CAN ADD MORE PARAMETERS HERE!!!
    )
{
    std::vector<int> predictions;
    const int num_images = images.size();
    predictions.reserve(num_images);

    // SNN-specific parameter, must match training
    const int T = 8;
    
    // 网络参数
    const int input_h = 28, input_w = 28, input_c = 1;
    const int conv1_out_c = 6, conv2_out_c = 16;
    const int fc1_out = 120, fc2_out = 84, fc3_out = 10;
    
    // 计算中间层尺寸
    const int conv1_out_h = (input_h + 2 * 0 - 5) / 1 + 1;  // 24
    const int conv1_out_w = (input_w + 2 * 0 - 5) / 1 + 1;  // 24
    const int pool1_out_h = conv1_out_h / 2;  // 12
    const int pool1_out_w = conv1_out_w / 2;  // 12
    const int conv2_out_h = (pool1_out_h + 2 * 0 - 5) / 1 + 1;  // 8
    const int conv2_out_w = (pool1_out_w + 2 * 0 - 5) / 1 + 1;  // 8
    const int pool2_out_h = conv2_out_h / 2;  // 4
    const int pool2_out_w = conv2_out_w / 2;  // 4
    const int fc1_input_size = conv2_out_c * pool2_out_h * pool2_out_w;  // 16*4*4 = 256
    
    // 分配GPU内存
    float *d_input, *d_conv1_out, *d_pool1_out, *d_conv2_out, *d_pool2_out;
    float *d_fc1_out, *d_fc2_out, *d_fc3_out, *d_final_out;
    float *d_membrane_conv1, *d_membrane_conv2, *d_membrane_fc1, *d_membrane_fc2;
    float *d_accumulated;
    
    // 分配输入输出内存
    checkCudaErrors(cudaMalloc(&d_input, input_h * input_w * input_c * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_conv1_out, conv1_out_h * conv1_out_w * conv1_out_c * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_pool1_out, pool1_out_h * pool1_out_w * conv1_out_c * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_conv2_out, conv2_out_h * conv2_out_w * conv2_out_c * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_pool2_out, pool2_out_h * pool2_out_w * conv2_out_c * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_fc1_out, fc1_out * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_fc2_out, fc2_out * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_fc3_out, fc3_out * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_final_out, fc3_out * sizeof(float)));
    
    // 分配膜电位内存
    checkCudaErrors(cudaMalloc(&d_membrane_conv1, conv1_out_h * conv1_out_w * conv1_out_c * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_membrane_conv2, conv2_out_h * conv2_out_w * conv2_out_c * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_membrane_fc1, fc1_out * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_membrane_fc2, fc2_out * sizeof(float)));
    checkCudaErrors(cudaMalloc(&d_accumulated, fc3_out * sizeof(float)));
    
    // 初始化膜电位为0
    checkCudaErrors(cudaMemset(d_membrane_conv1, 0, conv1_out_h * conv1_out_w * conv1_out_c * sizeof(float)));
    checkCudaErrors(cudaMemset(d_membrane_conv2, 0, conv2_out_h * conv2_out_w * conv2_out_c * sizeof(float)));
    checkCudaErrors(cudaMemset(d_membrane_fc1, 0, fc1_out * sizeof(float)));
    checkCudaErrors(cudaMemset(d_membrane_fc2, 0, fc2_out * sizeof(float)));
    checkCudaErrors(cudaMemset(d_accumulated, 0, fc3_out * sizeof(float)));
    
    // CUDA kernel配置
    const int block_size = 256;
    
    // --- Loop over each image ---
    for (int i = 0; i < num_images; ++i) {
        // 复制输入图像到GPU
        checkCudaErrors(cudaMemcpy(d_input, images[i].data(), 
                                  input_h * input_w * input_c * sizeof(float), 
                                  cudaMemcpyHostToDevice));
        
        // 重置膜电位
        checkCudaErrors(cudaMemset(d_membrane_conv1, 0, conv1_out_h * conv1_out_w * conv1_out_c * sizeof(float)));
        checkCudaErrors(cudaMemset(d_membrane_conv2, 0, conv2_out_h * conv2_out_w * conv2_out_c * sizeof(float)));
        checkCudaErrors(cudaMemset(d_membrane_fc1, 0, fc1_out * sizeof(float)));
        checkCudaErrors(cudaMemset(d_membrane_fc2, 0, fc2_out * sizeof(float)));
        checkCudaErrors(cudaMemset(d_accumulated, 0, fc3_out * sizeof(float)));
        
        // 时间步循环
        for (int t = 0; t < T; ++t) {
            // Conv1 + IF + MaxPool
            int conv1_size = conv1_out_h * conv1_out_w * conv1_out_c;
            int conv1_blocks = (conv1_size + block_size - 1) / block_size;
            conv2d_kernel<<<conv1_blocks, block_size>>>(
                d_input, d_conv1_w, d_conv1_b, d_conv1_out,
                input_h, input_w, input_c, conv1_out_c, 5, 1, 0
            );
            
            // IF神经元
            if_neuron_kernel<<<conv1_blocks, block_size>>>(
                d_conv1_out, d_conv1_out, d_membrane_conv1,
                conv1_size, 1.0f, 0.0f
            );
            
            // MaxPool
            int pool1_size = pool1_out_h * pool1_out_w * conv1_out_c;
            int pool1_blocks = (pool1_size + block_size - 1) / block_size;
            maxpool2d_kernel<<<pool1_blocks, block_size>>>(
                d_conv1_out, d_pool1_out,
                conv1_out_h, conv1_out_w, conv1_out_c, 2, 2
            );
            
            // Conv2 + IF + MaxPool
            int conv2_size = conv2_out_h * conv2_out_w * conv2_out_c;
            int conv2_blocks = (conv2_size + block_size - 1) / block_size;
            conv2d_kernel<<<conv2_blocks, block_size>>>(
                d_pool1_out, d_conv2_w, d_conv2_b, d_conv2_out,
                pool1_out_h, pool1_out_w, conv1_out_c, conv2_out_c, 5, 1, 0
            );
            
            // IF神经元
            if_neuron_kernel<<<conv2_blocks, block_size>>>(
                d_conv2_out, d_conv2_out, d_membrane_conv2,
                conv2_size, 1.0f, 0.0f
            );
            
            // MaxPool
            int pool2_size = pool2_out_h * pool2_out_w * conv2_out_c;
            int pool2_blocks = (pool2_size + block_size - 1) / block_size;
            maxpool2d_kernel<<<pool2_blocks, block_size>>>(
                d_conv2_out, d_pool2_out,
                conv2_out_h, conv2_out_w, conv2_out_c, 2, 2
            );
            
            // FC1 + IF
            int fc1_blocks = (fc1_out + block_size - 1) / block_size;
            linear_kernel<<<fc1_blocks, block_size>>>(
                d_pool2_out, d_fc1_w, d_fc1_b, d_fc1_out,
                fc1_input_size, fc1_out
            );
            
            // IF神经元
            if_neuron_kernel<<<fc1_blocks, block_size>>>(
                d_fc1_out, d_fc1_out, d_membrane_fc1,
                fc1_out, 1.0f, 0.0f
            );
            
            // FC2 + IF
            int fc2_blocks = (fc2_out + block_size - 1) / block_size;
            linear_kernel<<<fc2_blocks, block_size>>>(
                d_fc1_out, d_fc2_w, d_fc2_b, d_fc2_out,
                fc1_out, fc2_out
            );
            
            // IF神经元
            if_neuron_kernel<<<fc2_blocks, block_size>>>(
                d_fc2_out, d_fc2_out, d_membrane_fc2,
                fc2_out, 1.0f, 0.0f
            );
            
            // FC3
            int fc3_blocks = (fc3_out + block_size - 1) / block_size;
            linear_kernel<<<fc3_blocks, block_size>>>(
                d_fc2_out, d_fc3_w, d_fc3_b, d_fc3_out,
                fc2_out, fc3_out
            );
            
            // 累加到最终输出
            int accum_blocks = (fc3_out + block_size - 1) / block_size;
            accumulate_kernel<<<accum_blocks, block_size>>>(
                d_fc3_out, d_accumulated, fc3_out
            );
        }
        
        // 时间平均
        int final_blocks = (fc3_out + block_size - 1) / block_size;
        time_average_kernel<<<final_blocks, block_size>>>(
            d_accumulated, d_final_out, fc3_out, T
        );
        
        // 预测 - 在CPU上找到最大值索引
        float* h_final_out = new float[fc3_out];
        checkCudaErrors(cudaMemcpy(h_final_out, d_final_out, fc3_out * sizeof(float), cudaMemcpyDeviceToHost));
        
        int prediction = 0;
        float max_val = h_final_out[0];
        for (int j = 1; j < fc3_out; ++j) {
            if (h_final_out[j] > max_val) {
                max_val = h_final_out[j];
                prediction = j;
            }
        }
        
        delete[] h_final_out;
        predictions.push_back(prediction);
    }
    
    // 释放GPU内存
    checkCudaErrors(cudaFree(d_input));
    checkCudaErrors(cudaFree(d_conv1_out));
    checkCudaErrors(cudaFree(d_pool1_out));
    checkCudaErrors(cudaFree(d_conv2_out));
    checkCudaErrors(cudaFree(d_pool2_out));
    checkCudaErrors(cudaFree(d_fc1_out));
    checkCudaErrors(cudaFree(d_fc2_out));
    checkCudaErrors(cudaFree(d_fc3_out));
    checkCudaErrors(cudaFree(d_final_out));
    checkCudaErrors(cudaFree(d_membrane_conv1));
    checkCudaErrors(cudaFree(d_membrane_conv2));
    checkCudaErrors(cudaFree(d_membrane_fc1));
    checkCudaErrors(cudaFree(d_membrane_fc2));
    checkCudaErrors(cudaFree(d_accumulated));
    
    return predictions;
}

```

## 性能更好的实现代码在大作业一提交截至之后发布
反正每一年的作业都不一样，学弟们抄不了的。。。。